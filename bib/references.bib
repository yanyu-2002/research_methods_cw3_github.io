@article{ref1,
  title={{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Kulikov, Igor and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020},
  doi={10.48550/arXiv.2005.11401},
  url={https://arxiv.org/abs/2005.11401},
  abstract={Proposes Retrieval-Augmented Generation (RAG), integrating differentiable retrieval with pre-trained generation models for open-domain question answering and other knowledge-intensive NLP tasks.},
  keywords={retrieval-augmented generation, knowledge-intensive tasks, open-domain QA, LLM, external memory}
}

@article{ref2,
  title={{A Survey on the Memory Mechanism of Large Language Model based Agents}},
  author={Zhou, Qingyang and Zhu, Wenxuan and Sun, Kai},
  journal={arXiv preprint arXiv:2311.09029},
  year={2023},
  doi={10.48550/arXiv.2311.09029},
  url={https://arxiv.org/abs/2311.09029},
  note={arXiv preprint},
  abstract={Surveys memory mechanisms in LLM-based agents, categorizing them into internal, external, and episodic memory, and analyzing memory management and scheduling strategies.},
  keywords={LLM-agent, memory mechanism, long-term memory, memory types, scheduling}
}

@article{ref3,
  title={{Episodic Memory in Lifelong Language Learning}},
  author={d'Autume, Cyprien de Masson and Rockt{\"a}schel, Tim and Riedel, Sebastian and Yogatama, Dani},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={13143--13152},
  year={2019},
  doi={10.48550/arXiv.1906.01076},
  url={https://arxiv.org/abs/1906.01076},
  abstract={Introduces an episodic memory module to support continual learning in language models, improving resistance to catastrophic forgetting and enabling memory replay.},
  keywords={episodic memory, lifelong learning, continual learning, catastrophic forgetting, LLM}
}

@article{ref4,
  title={{Learning to Forget: Continual Prediction with LSTM}},
  author={Gers, Felix A. and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  journal={Neural Computation},
  volume={12},
  number={10},
  pages={2451--2471},
  year={2000},
  doi={10.1162/089976600300015015},
  url={https://doi.org/10.1162/089976600300015015},
  abstract={Proposes a forget gate in LSTM to regulate memory cell updates, preventing uncontrolled growth of internal state and improving sequence modeling.},
  keywords={LSTM, continual prediction, forget gate, recurrent networks, time-series memory}
}

@article{ref5,
  title={{Meta-Learning with Memory-Augmented Neural Networks}},
  author={Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  journal={Proceedings of the 33rd International Conference on Machine Learning (ICML)},
  year={2016},
  pages={1842--1850},
  url={https://arxiv.org/abs/1605.06065},
  note={ICML},
  abstract={Proposes MANNs (Memory-Augmented Neural Networks) that use external memory and meta-learning to perform rapid adaptation to new tasks from limited examples.},
  keywords={meta-learning, memory-augmented networks, few-shot learning, external memory, one-shot learning}
}

@article{ref6,
  title={{MemGPT: Towards LLMs as Operating Systems}},
  author={Weng, Lilian and Lin, Yuntao and Lewkowycz, Aitor and Chi, Ethan and Scales, Nathan and Hou, Ke and Zoph, Barret and Robinson, Mike},
  journal={arXiv preprint arXiv:2403.19490},
  year={2024},
  doi={10.48550/arXiv.2403.19490},
  url={https://arxiv.org/abs/2403.19490},
  note={arXiv preprint},
  abstract={Introduces MemGPT, a framework treating LLMs as operating systems with memory management capabilities, inspired by traditional OS concepts like paging and memory hierarchy.},
  keywords={LLM, operating systems, memory management, long-term memory, MemGPT}
}

@article{ref7,
  title={{Survey on Memory-Augmented Neural Networks: Cognitive Insights to AI Applications}},
  author={Zhu, Wenxuan and Zhou, Qingyang and Wang, Zhe and Zhao, Zhicheng},
  journal={arXiv preprint arXiv:2403.05784},
  year={2024},
  doi={10.48550/arXiv.2403.05784},
  url={https://arxiv.org/abs/2403.05784},
  note={arXiv preprint},
  abstract={Provides a survey of Memory-Augmented Neural Networks (MANNs), summarizing their architectures, learning algorithms, and cognitive inspirations for rapid generalization.},
  keywords={memory-augmented neural networks, MANN, cognitive modeling, neural memory, few-shot learning}
}

@article{ref8,
  title={{Toward Conversational Agents with Context and Time Sensitive Long-term Memory}},
  author={Lee, Kyungjae and Choi, Dongjun and Seo, Minjoon},
  journal={arXiv preprint arXiv:2401.10695},
  year={2024},
  doi={10.48550/arXiv.2401.10695},
  url={https://arxiv.org/abs/2401.10695},
  note={arXiv preprint},
  abstract={Presents memory mechanisms sensitive to both temporal and contextual signals to improve long-term memory retention in task-oriented dialogue systems.},
  keywords={dialog systems, context-aware memory, temporal memory, long-term memory, conversational agent}
}

@article{ref9,
  title={{Mitigating Catastrophic Forgetting in Long Short-Term Memory Networks}},
  author={Sodhani, Shagun and Chandar, Sarath and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22600--22610},
  year={2020},
  doi={10.48550/arXiv.1812.01718},
  url={https://arxiv.org/abs/1812.01718},
  abstract={Proposes state-decoupled LSTM to prevent catastrophic forgetting in sequential tasks without relying on replay or rehearsal, achieving continual learning.},
  keywords={catastrophic forgetting, continual learning, LSTM, decoupled memory, sequence modeling}
}

@article{ref10,
  title={{Extreme-Long-Short Term Memory for Time-series Prediction}},
  author={Xue, Ziyu and Wang, Lijun and Zhao, Zhongkai and Liu, Xue and Zhu, Quanying},
  journal={Information Sciences},
  volume={655},
  pages={120330},
  year={2023},
  doi={10.1016/j.ins.2023.120330},
  url={https://doi.org/10.1016/j.ins.2023.120330},
  abstract={Introduces Extreme-LSTM model capable of capturing ultra-long dependencies in time-series data by extending standard LSTM architectures with additional memory gates.},
  keywords={time-series prediction, LSTM, long-range dependency, memory mechanism, extreme memory}
}