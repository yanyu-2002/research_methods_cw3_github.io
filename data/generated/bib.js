const generatedBibEntries = {
    "ref1": {
        "abstract": "Proposes Retrieval-Augmented Generation (RAG), integrating differentiable retrieval with pre-trained generation models for open-domain question answering and other knowledge-intensive NLP tasks.",
        "author": "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Kulikov, Igor and Lewis, Mike and Yih, Wen-tau and Rockt{\\\"a}schel, Tim and Riedel, Sebastian and Zettlemoyer, Luke",
        "doi": "10.48550/arXiv.2005.11401",
        "journal": "Advances in Neural Information Processing Systems",
        "keywords": "retrieval-augmented generation, knowledge-intensive tasks, open-domain QA, LLM, external memory",
        "pages": "9459--9474",
        "title": "{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}",
        "type": "article",
        "url": "https://arxiv.org/abs/2005.11401",
        "volume": "33",
        "year": "2020"
    },
    "ref10": {
        "abstract": "Introduces Extreme-LSTM model capable of capturing ultra-long dependencies in time-series data by extending standard LSTM architectures with additional memory gates.",
        "author": "Xue, Ziyu and Wang, Lijun and Zhao, Zhongkai and Liu, Xue and Zhu, Quanying",
        "doi": "10.1016/j.ins.2023.120330",
        "journal": "Information Sciences",
        "keywords": "time-series prediction, LSTM, long-range dependency, memory mechanism, extreme memory",
        "pages": "120330",
        "title": "{Extreme-Long-Short Term Memory for Time-series Prediction}",
        "type": "article",
        "url": "https://doi.org/10.1016/j.ins.2023.120330",
        "volume": "655",
        "year": "2023"
    },
    "ref2": {
        "abstract": "Surveys memory mechanisms in LLM-based agents, categorizing them into internal, external, and episodic memory, and analyzing memory management and scheduling strategies.",
        "author": "Zhou, Qingyang and Zhu, Wenxuan and Sun, Kai",
        "doi": "10.48550/arXiv.2311.09029",
        "journal": "arXiv preprint arXiv:2311.09029",
        "keywords": "LLM-agent, memory mechanism, long-term memory, memory types, scheduling",
        "note": "arXiv preprint",
        "title": "{A Survey on the Memory Mechanism of Large Language Model based Agents}",
        "type": "article",
        "url": "https://arxiv.org/abs/2311.09029",
        "year": "2023"
    },
    "ref3": {
        "abstract": "Introduces an episodic memory module to support continual learning in language models, improving resistance to catastrophic forgetting and enabling memory replay.",
        "author": "d'Autume, Cyprien de Masson and Rockt{\\\"a}schel, Tim and Riedel, Sebastian and Yogatama, Dani",
        "doi": "10.48550/arXiv.1906.01076",
        "journal": "Advances in Neural Information Processing Systems",
        "keywords": "episodic memory, lifelong learning, continual learning, catastrophic forgetting, LLM",
        "pages": "13143--13152",
        "title": "{Episodic Memory in Lifelong Language Learning}",
        "type": "article",
        "url": "https://arxiv.org/abs/1906.01076",
        "volume": "32",
        "year": "2019"
    },
    "ref4": {
        "abstract": "Proposes a forget gate in LSTM to regulate memory cell updates, preventing uncontrolled growth of internal state and improving sequence modeling.",
        "author": "Gers, Felix A. and Schmidhuber, J{\\\"u}rgen and Cummins, Fred",
        "doi": "10.1162/089976600300015015",
        "journal": "Neural Computation",
        "keywords": "LSTM, continual prediction, forget gate, recurrent networks, time-series memory",
        "number": "10",
        "pages": "2451--2471",
        "title": "{Learning to Forget: Continual Prediction with LSTM}",
        "type": "article",
        "url": "https://doi.org/10.1162/089976600300015015",
        "volume": "12",
        "year": "2000"
    },
    "ref5": {
        "abstract": "Proposes MANNs (Memory-Augmented Neural Networks) that use external memory and meta-learning to perform rapid adaptation to new tasks from limited examples.",
        "author": "Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy",
        "journal": "Proceedings of the 33rd International Conference on Machine Learning (ICML)",
        "keywords": "meta-learning, memory-augmented networks, few-shot learning, external memory, one-shot learning",
        "note": "ICML",
        "pages": "1842--1850",
        "title": "{Meta-Learning with Memory-Augmented Neural Networks}",
        "type": "article",
        "url": "https://arxiv.org/abs/1605.06065",
        "year": "2016"
    },
    "ref6": {
        "abstract": "Introduces MemGPT, a framework treating LLMs as operating systems with memory management capabilities, inspired by traditional OS concepts like paging and memory hierarchy.",
        "author": "Weng, Lilian and Lin, Yuntao and Lewkowycz, Aitor and Chi, Ethan and Scales, Nathan and Hou, Ke and Zoph, Barret and Robinson, Mike",
        "doi": "10.48550/arXiv.2403.19490",
        "journal": "arXiv preprint arXiv:2403.19490",
        "keywords": "LLM, operating systems, memory management, long-term memory, MemGPT",
        "note": "arXiv preprint",
        "title": "{MemGPT: Towards LLMs as Operating Systems}",
        "type": "article",
        "url": "https://arxiv.org/abs/2403.19490",
        "year": "2024"
    },
    "ref7": {
        "abstract": "Provides a survey of Memory-Augmented Neural Networks (MANNs), summarizing their architectures, learning algorithms, and cognitive inspirations for rapid generalization.",
        "author": "Zhu, Wenxuan and Zhou, Qingyang and Wang, Zhe and Zhao, Zhicheng",
        "doi": "10.48550/arXiv.2403.05784",
        "journal": "arXiv preprint arXiv:2403.05784",
        "keywords": "memory-augmented neural networks, MANN, cognitive modeling, neural memory, few-shot learning",
        "note": "arXiv preprint",
        "title": "{Survey on Memory-Augmented Neural Networks: Cognitive Insights to AI Applications}",
        "type": "article",
        "url": "https://arxiv.org/abs/2403.05784",
        "year": "2024"
    },
    "ref8": {
        "abstract": "Presents memory mechanisms sensitive to both temporal and contextual signals to improve long-term memory retention in task-oriented dialogue systems.",
        "author": "Lee, Kyungjae and Choi, Dongjun and Seo, Minjoon",
        "doi": "10.48550/arXiv.2401.10695",
        "journal": "arXiv preprint arXiv:2401.10695",
        "keywords": "dialog systems, context-aware memory, temporal memory, long-term memory, conversational agent",
        "note": "arXiv preprint",
        "title": "{Toward Conversational Agents with Context and Time Sensitive Long-term Memory}",
        "type": "article",
        "url": "https://arxiv.org/abs/2401.10695",
        "year": "2024"
    },
    "ref9": {
        "abstract": "Proposes state-decoupled LSTM to prevent catastrophic forgetting in sequential tasks without relying on replay or rehearsal, achieving continual learning.",
        "author": "Sodhani, Shagun and Chandar, Sarath and Bengio, Yoshua",
        "doi": "10.48550/arXiv.1812.01718",
        "journal": "Advances in Neural Information Processing Systems",
        "keywords": "catastrophic forgetting, continual learning, LSTM, decoupled memory, sequence modeling",
        "pages": "22600--22610",
        "title": "{Mitigating Catastrophic Forgetting in Long Short-Term Memory Networks}",
        "type": "article",
        "url": "https://arxiv.org/abs/1812.01718",
        "volume": "33",
        "year": "2020"
    }
};